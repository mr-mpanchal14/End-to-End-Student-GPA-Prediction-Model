#basic imports
import sys
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Display all the columns of the dataframe
pd.pandas.set_option('display.max_columns',None)
%matplotlib inline
mpl.style.use('ggplot')
sns.set_style('white')
sns.set_palette('viridis')
mpl.rcParams['figure.figsize'] = 6,4

# Modelling imports
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc


#Common Model Algorithms
from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process
from xgboost import XGBClassifier
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor
from catboost import CatBoostRegressor
from xgboost import XGBRegressor


#ignore warnings
import warnings
warnings.filterwarnings('ignore')
print('-'*25)

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


!pip list


df = pd.read_csv('data\Student_performance_data _.csv')
df.head(10)


df.shape


df.info()


df.isnull().sum()


df.duplicated().sum()


df.nunique()


df.describe()


# Target Variable would be GPA
df.drop(['GradeClass','StudentID'], axis=1, inplace=True)
df.head()





df.columns





# Identify categorical columns: columns that are not numerical
categorical_feature = [feature for feature in df.columns if len(df[feature].unique()) <= 5]
print("Categorical Variables Count: {}".format(len(categorical_feature)))


# Custom labels for the categorical columns
custom_labels = {
    'Ethnicity': ['Caucasian', 'African American', 'Asian', 'Other'],
    'Age': [15, 16, 17, 18],
    'ParentalEducation': ['None', 'High School', 'Some College', 'Bachelor\'s', 'Higher'],
    'Tutoring': ['No', 'Yes'],
    'ParentalSupport': ['No', 'Low', 'Moderate', 'High', 'Very High'],
    'Extracurricular': ['No', 'Yes'],
    'Sports': ['No', 'Yes'],
    'Music': ['No', 'Yes'],
    'Volunteering': ['No', 'Yes'],
    'Gender': ['Male', 'Female']
}

# Plot countplots for each categorical column
for column in categorical_feature:
    plt.figure(figsize=(8, 5))
    sns.countplot(data=df, x=column)
    plt.title(f'Countplot of {column}')
    
    # Directly set custom labels
    labels = custom_labels[column]
    ticks = range(len(labels))
    plt.xticks(ticks=ticks, labels=labels)
    
    plt.tight_layout()
    plt.show()





discrete_feature = [feature for feature in df.columns if len(df[feature].unique()) < 25 and feature not in categorical_feature]
print("Discrete Variables Count: {}".format(len(discrete_feature)))





continuous_feature=[feature for feature in df.columns if feature not in discrete_feature+categorical_feature]
print("Continuous feature Count {}".format(len(continuous_feature)))


continuous_feature


df[continuous_feature].head()


# Lets analyse the continuous values by creating histograms to understand the distribution

for feature in continuous_feature:
    data = df.copy()
    sns.histplot(data = data, x = data[feature], bins = 30, kde = True)
    plt.xlabel(feature)
    plt.ylabel("Count")
    plt.title(feature)
    plt.show()


for feature in continuous_feature:
    data = df.copy()
    stats.probplot(df[feature], dist = "norm", plot = plt)
    plt.xlabel(feature)
    plt.ylabel("Count")
    plt.title(feature)
    plt.show()





for feature in continuous_feature:
    data = df.copy()
    sns.boxplot(data[feature])
    plt.xlabel(feature)
    plt.ylabel("Count")
    plt.title(feature)
    plt.show()





sns.pairplot(df)
plt.show()


sns.scatterplot(x = df['Absences'], y = df['GPA'], data = df)
plt.xlabel('Absentees')
plt.ylabel('GPA')
plt.title('Absentees vs GPA')
plt.show()


df_corr = df.corr(numeric_only = True)
cmap = sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

df_corr.style.background_gradient(cmap, axis=1)\
    .format(precision=3)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_caption("Hover to magify")\
    .set_table_styles(magnify())


# Calculate the correlation with the Grade Class and find the strongest correlation
gpa_class_corr = df.corr(numeric_only=True)['GPA'].drop('GPA')
strongest_positive_correlation = gpa_class_corr.idxmax()
strongest_positive_correlation_value = gpa_class_corr.max()
strongest_negative_correlation = gpa_class_corr.idxmin()
strongest_negative_correlation_value = gpa_class_corr.min()

print(f"The strongest positive correlation with the GPA is {strongest_positive_correlation} with a value of {strongest_positive_correlation_value:.2f}")
print(f"The strongest negative correlation with the GPA is {strongest_negative_correlation} with a value of {strongest_negative_correlation_value:.2f}")





df.head()


df.describe()





total_students = df['StudentID'].nunique()
print(f'Number of students: {total_students}')





full_gpa = df[df['GPA'] == 4]['GPA'].count()
print(f'Number of students with full GPA: {full_gpa}')





less_than_1gpa = df[df['GPA'] <= 1]['GPA'].count()
print(f'Number of students with less than 1 GPA: {less_than_1gpa}')





less_than_2gpa = df[(df['GPA'] > 1) & (df['GPA'] <= 2)]['GPA'].count()
print(f'Number of students with less than 2 GPA (but greater than 1 GPA): {less_than_2gpa}')





fig, axs = plt.subplots(1, 2, figsize=(15, 7))
plt.subplot(121)
sns.histplot(data=df,x='GPA',bins=30,kde=True,color='g')
plt.subplot(122)
sns.histplot(data=df,x='GPA',bins=30,kde=True,hue='Gender', legend = False)
# Directly set custom labels
labels = custom_labels['Gender']
plt.legend(title='Gender', loc='upper right', labels=labels)
plt.show()








data = df.copy()


continuous_feature


for feature in continuous_feature:
    data1 = df.copy()
    data1[feature] = np.log1p(data1[feature])
    #stats.probplot(df[feature], dist = "norm", plot = plt)
    stats.probplot(data1[feature], dist = "norm", plot = plt)
    plt.xlabel(feature)
    plt.ylabel("Count")
    plt.title(feature + 'Log Transformed')
    plt.show()





df.columns





X = df.drop('GPA', axis = 1)
y = df['GPA']


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


X_train.shape, X_test.shape


y_train.shape, y_test.shape





sc = StandardScaler()


preprocessor = ColumnTransformer([
    ('sscaler', sc, X.columns),
])


X_train_scaled = preprocessor.fit_transform(X_train)
X_train_scaled.shape


X_test_scaled = preprocessor.transform(X_test)
X_test_scaled.shape





def evaluate_model(true, predicted):
    mae = mean_absolute_error(true, predicted)
    mse = mean_squared_error(true, predicted)
    rmse = np.sqrt(mean_squared_error(true, predicted))
    r2_square = r2_score(true, predicted)
    return mae, rmse, r2_square


models = {
    "Linear Regression": LinearRegression(),
    "Lasso": Lasso(),
    "Ridge": Ridge(),
    "K-Neighbors Regressor": KNeighborsRegressor(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest Regressor": RandomForestRegressor(),
    "XGBRegressor": XGBRegressor(), 
    "CatBoosting Regressor": CatBoostRegressor(verbose=False),
    "AdaBoost Regressor": AdaBoostRegressor()
}
model_list = []
r2_list =[]

for i in range(len(list(models))):
    model = list(models.values())[i]
    model.fit(X_train_scaled, y_train) # Train model

    # Make predictions
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)
    
    # Evaluate Train and Test dataset
    model_train_mae , model_train_rmse, model_train_r2 = evaluate_model(y_train, y_train_pred)

    model_test_mae , model_test_rmse, model_test_r2 = evaluate_model(y_test, y_test_pred)

    
    print(list(models.keys())[i])
    model_list.append(list(models.keys())[i])
    
    print('Model performance for Training set')
    print("- Root Mean Squared Error: {:.4f}".format(model_train_rmse))
    print("- Mean Absolute Error: {:.4f}".format(model_train_mae))
    print("- R2 Score: {:.4f}".format(model_train_r2))

    print('----------------------------------')
    
    print('Model performance for Test set')
    print("- Root Mean Squared Error: {:.4f}".format(model_test_rmse))
    print("- Mean Absolute Error: {:.4f}".format(model_test_mae))
    print("- R2 Score: {:.4f}".format(model_test_r2))
    r2_list.append(model_test_r2)
    
    print('='*35)
    print('\n')


pd.DataFrame(list(zip(model_list, r2_list)), columns=['Model Name', 'R2_Score']).sort_values(by=["R2_Score"],ascending=False)





lin_model = LinearRegression(fit_intercept=True)
lin_model = lin_model.fit(X_train_scaled, y_train)
y_pred = lin_model.predict(X_test_scaled)
score = r2_score(y_test, y_pred)*100
print(" Accuracy of the model is %.2f" %score)





X_test_scaled[4]


lin_model.predict([[ 1.37285117, -1.0291761 , -0.84616672, -1.78504629, -1.08250988,
        1.45955945, -0.65799605, -1.02021491, -0.78293347,  1.51039849,
       -0.49705784, -0.43041146]])


y_test



